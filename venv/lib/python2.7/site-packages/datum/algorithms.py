from cornaconvertor.script_convert import *
from cornaconvertor.helpers import replace_kegg_with_hmdb

import constants as cs
import helpers as hlp


def convert_maven_to_required_df(path, algo):
    """
    This function takes in the csv file and the algorithm
    of the component to be performed.

    Parameters
    -----------
    path : string
        path to the input file
    algo : string
        algorithm name
    Returns
    --------
    Dataframe: Converted Dataframe to corresponding algo input format
    """
    df, logs = hlp.check_maven_df_with_olmonk(path)
    if df.empty:
        return df, logs
    function = cs.ALGO_DICT.get(algo)
    converted_df = globals()[function](path)
    return converted_df, logs


def get_maven_file_type(path):
    """
    This function checks if a file is a maven output file or not
    based on the columns in the csv file.

    Parameters
    -----------
    path : string
        path to the input file
    Returns
    String : file type
    --------

    """

    if hlp.validate_input_file(path):
        df = hlp.read_file(path)
        file_type = hlp.check_file_type(df)
        return file_type


def quant_create_fragment_mapping(converted_df, data_dict = None):
    """
    This is to create the fragment mapping file from the maven input file
    and the source database by mapping the compounds from Maven file to
    the source mapping file for Quant App.

    Parameters
    -----------
    converted_df: dataframe
        converted_df after the maven function

    Returns
    -------
    mapped_df: dataframe
        the mapped dataframe for all the compounds
    logs: dictionary
        dictionary containing missing components

    """
    logs = dict()
    src_df = hlp.read_file(cs.SOURCE_FILE_PATH)
    maven_comp_list = list(converted_df[cs.COMPONENT_NAME].unique())
    file_type = hlp.check_df_type(converted_df)
    if file_type == cs.MSMS:
        converted_df = converted_df.merge(src_df, left_on=cs.COMPONENT_NAME,
                                          right_on=cs.COMPONENT_NAME)\
                                        [[cs.COMPONENT_NAME,
                                          cs.UNLABELED_FRAGMENT]]
        converted_df.drop_duplicates([cs.COMPONENT_NAME,
                                      cs.UNLABELED_FRAGMENT], keep='first',
                                     inplace=True)
        mapped_df = converted_df.reset_index(drop = True)
        mapped_df = hlp.append_row_from_json(mapped_df, data_dict)
        updated_component_list = list(mapped_df[cs.COMPONENT_NAME].unique())
        logs[cs.MISSING_COMPONENTS] = hlp.find_missing_components(
            maven_comp_list, updated_component_list)
        return mapped_df, logs

    elif file_type == cs.MS:
        metab = converted_df[cs.COMPONENT_NAME].unique()
        metab_df = pd.DataFrame(metab)
        metab_df[cs.UNLABELED_FRAGMENT] = metab_df[0]
        metab_df = metab_df.rename(columns = {0:cs.COMPONENT_NAME})
        metab_df = hlp.append_row_from_json(metab_df, data_dict)
        updated_component_list = list(metab_df[cs.COMPONENT_NAME].unique())
        logs[cs.MISSING_COMPONENTS] = hlp.find_missing_components(
            maven_comp_list, updated_component_list)

        return metab_df, logs


def na_create_fragment_mapping(converted_df, data_dict = None):
    """
    This is to create the fragment mapping file from the maven input file
    and the source database by mapping the compounds from Maven file to
    the source mapping file for NA Component.

    Parameters
    -----------
    converted_df: dataframe
        converted dataframe for NA correction

    Returns
    -------
    mapped_df: dataframe
        the mapped dataframe for all the compounds
    logs: dictionary
    	dictionary containing missing components

    """
    logs = dict()
    src_df = hlp.read_file(cs.SOURCE_FILE_PATH_TEST)
    maven_comp_list = list(converted_df[cs.COMPONENT_NAME].unique())
    converted_df = converted_df.merge(src_df, left_on=cs.COMPONENT_NAME,
                                      right_on=cs.COMPONENT_NAME)\
                                    [[cs.COMPONENT_NAME, cs.UNLABELED_FRAGMENT,
                                    cs.FORMULA, cs.ISOTOPIC_TRACER, cs.PARENT_FORMULA]]
    converted_df.drop_duplicates([cs.COMPONENT_NAME, cs.UNLABELED_FRAGMENT], keep='first',
                                 inplace=True)
    mapped_df = converted_df.reset_index(drop = True)
    mapped_df = hlp.append_row_from_json(mapped_df, data_dict)
    updated_component_list = list(mapped_df[cs.COMPONENT_NAME].unique())
    logs[cs.MISSING_COMPONENTS] = hlp.find_missing_components_na(
        maven_comp_list, updated_component_list)
    return mapped_df, logs


def pvd_create_metabolite_mapping(input_df):
    """
    This is to create the hmdb mapping file from the maven input file
    and the source database by mapping the compounds from Maven file to
    the source mapping file for Unlabeled PVD.

    Parameters
    -----------
    input_df: dataframe
        converted dataframe for PVD

    Returns
    -------
    sample_df: dataframe
        the mapped dataframe for all the compounds
    """
    metadata_df = hlp.read_file(cs.COMP_HMDB_FILE)
    df = input_df.dropna(how='all')
    if not df[cs.SMALL_L_LABEL_COLUMN].isnull().all():
        df = df[df[cs.SMALL_L_LABEL_COLUMN] == "g"]
    good_peak_df = df[df[cs.COMPOUNDID_COLUMN].str.contains(cs.HMDB)]
    hmdb_index = df.index.difference(good_peak_df.index)
    left_df = df.ix[hmdb_index, :]
    kegg_df = left_df[left_df[cs.COMPOUNDID_COLUMN].str.contains("^C[0-9]{5,"
                                                                    "}$")]
    kegg_list = list(kegg_df[cs.COMPOUNDID_COLUMN])
    hmdb_list = replace_kegg_with_hmdb(metadata_df, kegg_list)
    for f, b in zip(kegg_list, hmdb_list):
        kegg_df[cs.COMPOUNDID_COLUMN].replace(to_replace=f, value=b,
                                                 inplace=True)
    final = pd.concat([good_peak_df, kegg_df], ignore_index=True)
    sample_df = final[final[const.COMPOUNDID_COLUMN].str.contains(
        const.HMDB)]
    sample_df = sample_df[[cs.COMPOUND, cs.COMPOUNDID_COLUMN]]
    sample_df[cs.ID] = sample_df[const.COMPOUNDID_COLUMN]
    sample_df.insert(2, cs.KEGG_ID, 0)
    sample_df.rename(columns={cs.COMPOUND: cs.NAME, cs.COMPOUNDID_COLUMN:
        cs.HMDBID_COLUMN}, inplace=True)
    return sample_df
